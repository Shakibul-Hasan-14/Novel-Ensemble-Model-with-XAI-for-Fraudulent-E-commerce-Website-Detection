{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fd0a9f",
   "metadata": {},
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72a6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1a7f1",
   "metadata": {},
   "source": [
    "**Load and Read Train Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a9a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/shakib/Downloads/Train Data.csv\"\n",
    "train_data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45554bcb",
   "metadata": {},
   "source": [
    "**1. Extract Label and Features** <br>\n",
    "**2. Convert to Numpy Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7907dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=['Label'])\n",
    "y = train_data['Label']\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0a150",
   "metadata": {},
   "source": [
    "**Define K-Fold Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e096d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1f50e",
   "metadata": {},
   "source": [
    "**Train and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ef776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 1 Accuracy: 0.8900\n",
      "DT 1 Accuracy: 0.8900\n",
      "SVM 1 Accuracy: 0.9350\n",
      "KNN 1 Accuracy: 0.9100\n",
      "LR 2 Accuracy: 0.9050\n",
      "DT 2 Accuracy: 0.9450\n",
      "SVM 2 Accuracy: 0.9450\n",
      "KNN 2 Accuracy: 0.9350\n",
      "LR 3 Accuracy: 0.9000\n",
      "DT 3 Accuracy: 0.9150\n",
      "SVM 3 Accuracy: 0.9400\n",
      "KNN 3 Accuracy: 0.8850\n",
      "LR 4 Accuracy: 0.8550\n",
      "DT 4 Accuracy: 0.8800\n",
      "SVM 4 Accuracy: 0.9200\n",
      "KNN 4 Accuracy: 0.8500\n",
      "LR 5 Accuracy: 0.8650\n",
      "DT 5 Accuracy: 0.9150\n",
      "SVM 5 Accuracy: 0.9150\n",
      "KNN 5 Accuracy: 0.8850\n",
      "\n",
      "Mean K-Fold LR Accuracy: 0.8830\n",
      "\n",
      "Mean K-Fold DT Accuracy: 0.9090\n",
      "\n",
      "Mean K-Fold SVM Accuracy: 0.9310\n",
      "\n",
      "Mean K-Fold KNN Accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track results\n",
    "LR_accuracy = []\n",
    "DT_accuracy = []\n",
    "SVM_accuracy = []\n",
    "KNN_accuracy = []\n",
    "\n",
    "# K-fold cross-validation loop\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y), 1):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train models\n",
    "    LR = LogisticRegression(C=0.01, solver='liblinear')\n",
    "    LR.fit(X_train, y_train)\n",
    "    \n",
    "    DT = DecisionTreeClassifier(max_depth=4)\n",
    "    DT.fit(X_train, y_train)\n",
    "    \n",
    "    SVM = SVC(max_iter=18000)\n",
    "    SVM.fit(X_train, y_train)\n",
    "    \n",
    "    KNN = KNeighborsClassifier(n_neighbors = 10)\n",
    "    KNN.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    LR_y_hat = LR.predict(X_test)\n",
    "    DT_y_hat = DT.predict(X_test)\n",
    "    SVM_y_hat = SVM.predict(X_test)\n",
    "    KNN_y_hat = KNN.predict(X_test)\n",
    "                            \n",
    "    # Compute accuracy\n",
    "    LR_accuracy.append(accuracy_score(y_test, LR_y_hat))\n",
    "    DT_accuracy.append(accuracy_score(y_test, DT_y_hat))\n",
    "    SVM_accuracy.append(accuracy_score(y_test, SVM_y_hat))\n",
    "    KNN_accuracy.append(accuracy_score(y_test, KNN_y_hat))\n",
    "    \n",
    "    # Print accuracy for the current fold\n",
    "    print(f\"LR {fold} Accuracy: {accuracy_score(y_test, LR_y_hat):.4f}\")\n",
    "    print(f\"DT {fold} Accuracy: {accuracy_score(y_test, DT_y_hat):.4f}\")\n",
    "    print(f\"SVM {fold} Accuracy: {accuracy_score(y_test, SVM_y_hat):.4f}\")\n",
    "    print(f\"KNN {fold} Accuracy: {accuracy_score(y_test, KNN_y_hat):.4f}\")\n",
    "    \n",
    "# Print overall K-Fold accuracies\n",
    "LR_mean_accuracy = np.mean(LR_accuracy)\n",
    "print(f\"\\nMean K-Fold LR Accuracy: {LR_mean_accuracy:.4f}\")\n",
    "\n",
    "DT_mean_accuracy = np.mean(DT_accuracy)\n",
    "print(f\"\\nMean K-Fold DT Accuracy: {DT_mean_accuracy:.4f}\")\n",
    "\n",
    "SVM_mean_accuracy = np.mean(SVM_accuracy)\n",
    "print(f\"\\nMean K-Fold SVM Accuracy: {SVM_mean_accuracy:.4f}\")\n",
    "\n",
    "KNN_mean_accuracy = np.mean(KNN_accuracy)\n",
    "print(f\"\\nMean K-Fold KNN Accuracy: {KNN_mean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558236a",
   "metadata": {},
   "source": [
    "**Extract Label and Features for Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85eed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor(train_data['Label'].values, dtype=torch.long)\n",
    "features = torch.tensor(train_data.drop(columns=['Label']).values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6496b",
   "metadata": {},
   "source": [
    "**Define Feedforward Neural Network (FNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4283ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492adebe",
   "metadata": {},
   "source": [
    "**Initialize Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17b9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = features.shape[1]\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef225f",
   "metadata": {},
   "source": [
    "**Train and Predict Feedforward Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cf5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Validation Loss: 0.4734\n",
      "Epoch [2/30], Validation Loss: 0.2568\n",
      "Epoch [3/30], Validation Loss: 0.2201\n",
      "Epoch [4/30], Validation Loss: 0.2052\n",
      "Epoch [5/30], Validation Loss: 0.2099\n",
      "Epoch [6/30], Validation Loss: 0.1936\n",
      "Epoch [7/30], Validation Loss: 0.1864\n",
      "Epoch [8/30], Validation Loss: 0.2132\n",
      "Epoch [9/30], Validation Loss: 0.1877\n",
      "Epoch [10/30], Validation Loss: 0.1806\n",
      "Epoch [11/30], Validation Loss: 0.1774\n",
      "Epoch [12/30], Validation Loss: 0.1756\n",
      "Epoch [13/30], Validation Loss: 0.1840\n",
      "Epoch [14/30], Validation Loss: 0.1731\n",
      "Epoch [15/30], Validation Loss: 0.1724\n",
      "Epoch [16/30], Validation Loss: 0.1815\n",
      "Epoch [17/30], Validation Loss: 0.1770\n",
      "Epoch [18/30], Validation Loss: 0.1778\n",
      "Epoch [19/30], Validation Loss: 0.1693\n",
      "Epoch [20/30], Validation Loss: 0.1800\n",
      "Epoch [21/30], Validation Loss: 0.1733\n",
      "Epoch [22/30], Validation Loss: 0.1687\n",
      "Epoch [23/30], Validation Loss: 0.1728\n",
      "Epoch [24/30], Validation Loss: 0.1786\n",
      "Epoch [25/30], Validation Loss: 0.1803\n",
      "Epoch [26/30], Validation Loss: 0.1708\n",
      "Epoch [27/30], Validation Loss: 0.1734\n",
      "Epoch [28/30], Validation Loss: 0.1870\n",
      "Epoch [29/30], Validation Loss: 0.1690\n",
      "Epoch [30/30], Validation Loss: 0.1719\n",
      "Epoch [1/30], Validation Loss: 0.3976\n",
      "Epoch [2/30], Validation Loss: 0.1992\n",
      "Epoch [3/30], Validation Loss: 0.1612\n",
      "Epoch [4/30], Validation Loss: 0.1674\n",
      "Epoch [5/30], Validation Loss: 0.1459\n",
      "Epoch [6/30], Validation Loss: 0.1525\n",
      "Epoch [7/30], Validation Loss: 0.1313\n",
      "Epoch [8/30], Validation Loss: 0.1311\n",
      "Epoch [9/30], Validation Loss: 0.1188\n",
      "Epoch [10/30], Validation Loss: 0.1455\n",
      "Epoch [11/30], Validation Loss: 0.1103\n",
      "Epoch [12/30], Validation Loss: 0.1137\n",
      "Epoch [13/30], Validation Loss: 0.1065\n",
      "Epoch [14/30], Validation Loss: 0.1030\n",
      "Epoch [15/30], Validation Loss: 0.1035\n",
      "Epoch [16/30], Validation Loss: 0.1148\n",
      "Epoch [17/30], Validation Loss: 0.1100\n",
      "Epoch [18/30], Validation Loss: 0.1176\n",
      "Epoch [19/30], Validation Loss: 0.0986\n",
      "Epoch [20/30], Validation Loss: 0.1114\n",
      "Epoch [21/30], Validation Loss: 0.1039\n",
      "Epoch [22/30], Validation Loss: 0.0920\n",
      "Epoch [23/30], Validation Loss: 0.1039\n",
      "Epoch [24/30], Validation Loss: 0.0969\n",
      "Epoch [25/30], Validation Loss: 0.1109\n",
      "Epoch [26/30], Validation Loss: 0.0971\n",
      "Epoch [27/30], Validation Loss: 0.0904\n",
      "Epoch [28/30], Validation Loss: 0.0942\n",
      "Epoch [29/30], Validation Loss: 0.1087\n",
      "Epoch [30/30], Validation Loss: 0.0904\n",
      "Epoch [1/30], Validation Loss: 0.4108\n",
      "Epoch [2/30], Validation Loss: 0.2393\n",
      "Epoch [3/30], Validation Loss: 0.2122\n",
      "Epoch [4/30], Validation Loss: 0.1981\n",
      "Epoch [5/30], Validation Loss: 0.1822\n",
      "Epoch [6/30], Validation Loss: 0.1786\n",
      "Epoch [7/30], Validation Loss: 0.1671\n",
      "Epoch [8/30], Validation Loss: 0.1866\n",
      "Epoch [9/30], Validation Loss: 0.1975\n",
      "Epoch [10/30], Validation Loss: 0.1884\n",
      "Epoch [11/30], Validation Loss: 0.1728\n",
      "Epoch [12/30], Validation Loss: 0.2120\n",
      "Epoch [13/30], Validation Loss: 0.1701\n",
      "Epoch [14/30], Validation Loss: 0.1668\n",
      "Epoch [15/30], Validation Loss: 0.1738\n",
      "Epoch [16/30], Validation Loss: 0.1651\n",
      "Epoch [17/30], Validation Loss: 0.1925\n",
      "Epoch [18/30], Validation Loss: 0.2134\n",
      "Epoch [19/30], Validation Loss: 0.2075\n",
      "Epoch [20/30], Validation Loss: 0.1909\n",
      "Epoch [21/30], Validation Loss: 0.1649\n",
      "Epoch [22/30], Validation Loss: 0.1687\n",
      "Epoch [23/30], Validation Loss: 0.2155\n",
      "Epoch [24/30], Validation Loss: 0.1928\n",
      "Epoch [25/30], Validation Loss: 0.1870\n",
      "Epoch [26/30], Validation Loss: 0.1830\n",
      "Epoch [27/30], Validation Loss: 0.2256\n",
      "Epoch [28/30], Validation Loss: 0.2248\n",
      "Epoch [29/30], Validation Loss: 0.2113\n",
      "Epoch [30/30], Validation Loss: 0.2087\n",
      "Epoch [1/30], Validation Loss: 0.4645\n",
      "Epoch [2/30], Validation Loss: 0.3182\n",
      "Epoch [3/30], Validation Loss: 0.2902\n",
      "Epoch [4/30], Validation Loss: 0.2698\n",
      "Epoch [5/30], Validation Loss: 0.2625\n",
      "Epoch [6/30], Validation Loss: 0.2544\n",
      "Epoch [7/30], Validation Loss: 0.2457\n",
      "Epoch [8/30], Validation Loss: 0.2372\n",
      "Epoch [9/30], Validation Loss: 0.2373\n",
      "Epoch [10/30], Validation Loss: 0.2451\n",
      "Epoch [11/30], Validation Loss: 0.2311\n",
      "Epoch [12/30], Validation Loss: 0.2703\n",
      "Epoch [13/30], Validation Loss: 0.2310\n",
      "Epoch [14/30], Validation Loss: 0.2315\n",
      "Epoch [15/30], Validation Loss: 0.2299\n",
      "Epoch [16/30], Validation Loss: 0.2545\n",
      "Epoch [17/30], Validation Loss: 0.2151\n",
      "Epoch [18/30], Validation Loss: 0.2197\n",
      "Epoch [19/30], Validation Loss: 0.2243\n",
      "Epoch [20/30], Validation Loss: 0.2281\n",
      "Epoch [21/30], Validation Loss: 0.2268\n",
      "Epoch [22/30], Validation Loss: 0.2501\n",
      "Epoch [23/30], Validation Loss: 0.2233\n",
      "Epoch [24/30], Validation Loss: 0.2282\n",
      "Epoch [25/30], Validation Loss: 0.2457\n",
      "Epoch [26/30], Validation Loss: 0.2276\n",
      "Epoch [27/30], Validation Loss: 0.2303\n",
      "Early stopping at epoch: 27\n",
      "Epoch [1/30], Validation Loss: 0.4780\n",
      "Epoch [2/30], Validation Loss: 0.2596\n",
      "Epoch [3/30], Validation Loss: 0.2386\n",
      "Epoch [4/30], Validation Loss: 0.2110\n",
      "Epoch [5/30], Validation Loss: 0.2087\n",
      "Epoch [6/30], Validation Loss: 0.2086\n",
      "Epoch [7/30], Validation Loss: 0.2068\n",
      "Epoch [8/30], Validation Loss: 0.2098\n",
      "Epoch [9/30], Validation Loss: 0.2397\n",
      "Epoch [10/30], Validation Loss: 0.2166\n",
      "Epoch [11/30], Validation Loss: 0.2112\n",
      "Epoch [12/30], Validation Loss: 0.2053\n",
      "Epoch [13/30], Validation Loss: 0.2126\n",
      "Epoch [14/30], Validation Loss: 0.2098\n",
      "Epoch [15/30], Validation Loss: 0.2294\n",
      "Epoch [16/30], Validation Loss: 0.2135\n",
      "Epoch [17/30], Validation Loss: 0.2171\n",
      "Epoch [18/30], Validation Loss: 0.2118\n",
      "Epoch [19/30], Validation Loss: 0.2322\n",
      "Epoch [20/30], Validation Loss: 0.2222\n",
      "Epoch [21/30], Validation Loss: 0.2230\n",
      "Epoch [22/30], Validation Loss: 0.2243\n",
      "Early stopping at epoch: 22\n",
      "Fold 1 - Validation Loss: 0.1687\n",
      "Fold 2 - Validation Loss: 0.0904\n",
      "Fold 3 - Validation Loss: 0.1649\n",
      "Fold 4 - Validation Loss: 0.2151\n",
      "Fold 5 - Validation Loss: 0.2053\n",
      "Average Validation Loss across all folds: 0.1689\n",
      "\n",
      "Mean K-Fold FNN Accuracy: 0.9520\n"
     ]
    }
   ],
   "source": [
    "fold_results = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(features)):\n",
    "    # Split data\n",
    "    features_train, features_val = features[train_index], features[val_index]\n",
    "    labels_train, labels_val = labels[train_index], labels[val_index]\n",
    "    \n",
    "    # Initialize model and optimizer for each fold\n",
    "    NN = FNN(input_dim=input_dim)\n",
    "    optimizer = optim.Adam(NN.parameters(), lr=0.001)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "    patience = 10  # Number of epochs to wait for improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        NN.train()\n",
    "        # Shuffle training data\n",
    "        permutation = torch.randperm(features_train.size()[0])\n",
    "        \n",
    "        for i in range(0, features_train.size()[0], batch_size):\n",
    "            # Get the current batch\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_features, batch_labels = features_train[indices], labels_train[indices].float().view(-1, 1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = NN(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation check\n",
    "        NN.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = NN(features_val)\n",
    "            val_loss = criterion(val_outputs, labels_val.float().view(-1, 1))\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                trigger_times = 0  # Reset patience counter\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                if trigger_times >= patience:\n",
    "                    print(f\"Early stopping at epoch: {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "    fold_results.append(best_val_loss.item())  # Store the validation loss for each fold\n",
    "\n",
    "# Print results for each fold\n",
    "for i, val_loss in enumerate(fold_results):\n",
    "    print(f\"Fold {i + 1} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Calculate average validation loss across all folds\n",
    "average_val_loss = sum(fold_results) / len(fold_results)\n",
    "print(f\"Average Validation Loss across all folds: {average_val_loss:.4f}\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "NN.eval()\n",
    "\n",
    "# Forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = NN(features)\n",
    "    predicted = (outputs >= 0.5).float()  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "FNN_train_accuracy = accuracy_score(labels, predicted)\n",
    "print(f\"\\nMean K-Fold FNN Accuracy: {FNN_train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f688e",
   "metadata": {},
   "source": [
    "**Load and Read Unseen Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282b5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"C:/Users/shakib/Downloads/Unseen Data.csv\"\n",
    "unseen_data = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f4cf1",
   "metadata": {},
   "source": [
    "**Extract Label and Features from Unseen Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3baed2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unseen = unseen_data['Label']\n",
    "X_unseen = np.asarray(unseen_data.drop(columns=['Label']))\n",
    "\n",
    "# For Neural Network\n",
    "X_value = unseen_data.drop(columns=['Label']).values\n",
    "X_unseen_tensor = torch.tensor(X_value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c66e74",
   "metadata": {},
   "source": [
    "**Predict on Unseen Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efab751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_y_unseen_pred = LR.predict(X_unseen)\n",
    "LR_accuracy = accuracy_score(y_unseen, LR_y_unseen_pred)\n",
    "\n",
    "DT_y_unseen_pred = DT.predict(X_unseen)\n",
    "DT_accuracy = accuracy_score(y_unseen, DT_y_unseen_pred)\n",
    "\n",
    "SVM_y_unseen_pred = SVM.predict(X_unseen)\n",
    "SVM_accuracy = accuracy_score(y_unseen, SVM_y_unseen_pred)\n",
    "\n",
    "KNN_y_unseen_pred = KNN.predict(X_unseen)\n",
    "KNN_accuracy = accuracy_score(y_unseen, KNN_y_unseen_pred)\n",
    "\n",
    "NN.eval()\n",
    "with torch.no_grad():\n",
    "    fnn_y_unseen_pred = NN(X_unseen_tensor).numpy().flatten()\n",
    "\n",
    "# Threshold predictions to get binary labels\n",
    "fnn_y_unseen_pred = (fnn_y_unseen_pred > 0.5).astype(int)\n",
    "\n",
    "FNN_accuracy = accuracy_score(y_unseen, fnn_y_unseen_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a547d92",
   "metadata": {},
   "source": [
    "**Results Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd168eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base Models</th>\n",
       "      <th>Train Data Accuracy</th>\n",
       "      <th>Unseen Data Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>88.30%</td>\n",
       "      <td>82.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>90.90%</td>\n",
       "      <td>88.57%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>93.10%</td>\n",
       "      <td>90.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>89.30%</td>\n",
       "      <td>86.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feedforward Neural Network</td>\n",
       "      <td>95.20%</td>\n",
       "      <td>87.86%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Base Models Train Data Accuracy Unseen Data Accuracy\n",
       "0         Logistic Regression              88.30%               82.86%\n",
       "1               Decision Tree              90.90%               88.57%\n",
       "2      Support Vector Machine              93.10%               90.00%\n",
       "3         K-Nearest Neighbors              89.30%               86.43%\n",
       "4  Feedforward Neural Network              95.20%               87.86%"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Base Models': [\n",
    "        'Logistic Regression', \n",
    "        'Decision Tree', \n",
    "        'Support Vector Machine', \n",
    "        'K-Nearest Neighbors', \n",
    "        'Feedforward Neural Network'\n",
    "    ],\n",
    "    'Train Data Accuracy': [\n",
    "        f\"{LR_mean_accuracy * 100:.2f}%\", \n",
    "        f\"{DT_mean_accuracy * 100:.2f}%\", \n",
    "        f\"{SVM_mean_accuracy * 100:.2f}%\", \n",
    "        f\"{KNN_mean_accuracy * 100:.2f}%\", \n",
    "        f\"{FNN_train_accuracy * 100:.2f}%\"\n",
    "    ],\n",
    "    'Unseen Data Accuracy': [\n",
    "        f\"{LR_accuracy * 100:.2f}%\", \n",
    "        f\"{DT_accuracy * 100:.2f}%\", \n",
    "        f\"{SVM_accuracy * 100:.2f}%\", \n",
    "        f\"{KNN_accuracy * 100:.2f}%\", \n",
    "        f\"{FNN_accuracy * 100:.2f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr tf2.4 py3.8",
   "language": "python",
   "name": "cvpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
